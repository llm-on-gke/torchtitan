# vcjob-quickstart.yaml
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: a3-mega
spec:
  minAvailable: 1
  schedulerName: volcano
  # If you omit the 'queue' field, the 'default' queue will be used.
  # queue: default
  policies:
   - event: TaskCompleted
     action: CompleteJob
  plugins:
    pytorch:
      - --master=master
      - --worker=worker
      #- --port=3389  
  tasks:
    - replicas: 1
      name: master
      policies:
      # When this specific task completes successfully, mark the entire job as Complete.
      - event: TaskCompleted
        action: CompleteJob
      template:
        metadata:
             annotations:
               gke-gcsfuse/volumes: "true"
        spec:
          nodeSelector:
             cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb #for non dws
          tolerations:
               - key: "nvidia.com/gpu"
                 operator: "Exists"
                 effect: "NoSchedule"
          volumes:
            - name: libraries
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: sys
              hostPath:
               path: /sys
            - name: proc-sys
              hostPath:
               path: /proc/sys
            - name: aperture-devices
              hostPath:
               path: /dev/aperture_devices
            - name: gcs-fuse-csi-ephemeral
              csi:
                driver: gcsfuse.csi.storage.gke.io
                readOnly: false
                volumeAttributes:
                   bucketName: rick-llama-factory
                   mountOptions: "implicit-dirs"
                   gcsfuseLoggingSeverity: warning
                   fileCacheCapacity: "200Gi"
            - name: dshm
              emptyDir:
                medium: Memory
          serviceAccountName: storage-access
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          initContainers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.17
              imagePullPolicy: IfNotPresent
              restartPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                - |
                  set -ex
                  chmod 755 /fts/entrypoint_rxdm_container.sh
                  /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
                  exit 0
              securityContext:
                privileged: true
                
              volumeMounts:
               - name: libraries
                 mountPath: /usr/local/nvidia
               - name: sys
                 mountPath: /hostsysfs
               - name: proc-sys
                 mountPath: /hostprocsysfs
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64
             
          containers:
           
            - image: us-east4-docker.pkg.dev/northam-ce-mlai-tpu/gke-llm/torchtitan:latest
              name: master
              command:
              - bash
              - -xc
              - |
                NCCL_LIB_DIR="/usr/local/nvidia/lib64"
                export MASTER_ADDR=3389
                source ${NCCL_LIB_DIR}/nccl-env-profile.sh
                #sleep infinity
                CONFIG_FILE="./torchtitan/models/deepseek_v3/train_configs/debug_model.toml" ./run_train.sh
                
              env:
               - name: LD_LIBRARY_PATH
                 value: /usr/local/nvidia/lib64
               - name: NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY
                 value: /dev/aperture_devices
              securityContext:
              volumeMounts:
               - name: aperture-devices
                 mountPath: /dev/aperture_devices
               - name: libraries
                 mountPath: /usr/local/nvidia
               - mountPath: /dev/shm
                 name: dshm
               - mountPath: /gcs-dir
                 name: gcs-fuse-csi-ephemeral
          
              resources:
                requests:
                  #cpu: 1
                  nvidia.com/gpu: 8
                limits:
                  #cpu: 1
                  nvidia.com/gpu: 8

          restartPolicy: OnFailure
    - replicas: 1
      name: worker
      #policies:
      # When this specific task completes successfully, mark the entire job as Complete.
      #- event: TaskCompleted
      #  action: CompleteJob
      template:
        metadata:
             annotations:
               gke-gcsfuse/volumes: "true"
        spec:
          nodeSelector:
             cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb #for non dws
          tolerations:
               - key: "nvidia.com/gpu"
                 operator: "Exists"
                 effect: "NoSchedule"
          volumes:
            - name: libraries
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: sys
              hostPath:
               path: /sys
            - name: proc-sys
              hostPath:
               path: /proc/sys
            - name: aperture-devices
              hostPath:
               path: /dev/aperture_devices
            - name: gcs-fuse-csi-ephemeral
              csi:
                driver: gcsfuse.csi.storage.gke.io
                readOnly: false
                volumeAttributes:
                   bucketName: rick-llama-factory
                   mountOptions: "implicit-dirs"
                   gcsfuseLoggingSeverity: warning
                   fileCacheCapacity: "200Gi"
            - name: dshm
              emptyDir:
                medium: Memory
          serviceAccountName: storage-access
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          initContainers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.17
              imagePullPolicy: Always
              restartPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                - |
                  set -ex
                  chmod 755 /fts/entrypoint_rxdm_container.sh
                  /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
                  exit 0
              securityContext:
                privileged: true   
              volumeMounts:
               - name: libraries
                 mountPath: /usr/local/nvidia
               - name: sys
                 mountPath: /hostsysfs
               - name: proc-sys
                 mountPath: /hostprocsysfs
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64
          containers:
            - image: us-east4-docker.pkg.dev/northam-ce-mlai-tpu/gke-llm/torchtitan:latest
              name: worker
              command:
              - bash
              - -xc
              - |
                NCCL_LIB_DIR="/usr/local/nvidia/lib64"
                export MASTER_ADDR=3389
                source ${NCCL_LIB_DIR}/nccl-env-profile.sh
                #sleep infinity
                CONFIG_FILE="./torchtitan/models/deepseek_v3/train_configs/debug_model.toml" ./run_train.sh  
              env:
               - name: LD_LIBRARY_PATH
                 value: /usr/local/nvidia/lib64
               - name: NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY
                 value: /dev/aperture_devices
              securityContext:
              volumeMounts:
               - name: aperture-devices
                 mountPath: /dev/aperture_devices
               - name: libraries
                 mountPath: /usr/local/nvidia
               - mountPath: /dev/shm
                 name: dshm
               - mountPath: /gcs-dir
                 name: gcs-fuse-csi-ephemeral
          
              resources:
                requests:
                  #cpu: 1
                  nvidia.com/gpu: 8
                limits:
                  #cpu: 1
                  nvidia.com/gpu: 8

          restartPolicy: OnFailure 